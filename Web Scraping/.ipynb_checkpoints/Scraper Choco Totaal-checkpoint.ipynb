{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a63e3c82",
   "metadata": {},
   "source": [
    "# Albert Heijn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd230d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved to products.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import csv\n",
    "import re  # Importing the regular expression module\n",
    "from datetime import datetime  # Importing datetime for timestamp\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=Options())\n",
    "\n",
    "\n",
    "url = \"https://www.ah.nl/producten/snoep-chocolade-koek/chocolade/chocoladesnoepjes?merk=AH&kenmerk=prijsfavoriet\"\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "\n",
    "# Borrowed the 'very cool :)' accepteer cookies button from TOTO scraper(group assignment)\n",
    "accept_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, \"decline-cookies\")))\n",
    "accept_button.click()\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# List to store the extracted product information\n",
    "products = []\n",
    "\n",
    "# Loop through all product articles\n",
    "for article in soup.find_all('article', class_='product-card-portrait_root__ZiRpZ'):\n",
    "    # Extract the price from the aria-label of the sr-only span\n",
    "    price_span = article.find('span', class_='sr-only')\n",
    "    if price_span:\n",
    "        # Use regular expression to extract the numeric price (e.g., 1.99)\n",
    "        match = re.search(r'[\\d]+[.,][\\d]+', price_span.get('aria-label'))\n",
    "        price = match.group() if match else 'Price not found'\n",
    "    else:\n",
    "        price = 'Price not found'\n",
    "        \n",
    "    # Extract the promo price (if available) from the correct div\n",
    "    promo_price_span = article.find('div', class_='price-amount_highlight__ekL92')\n",
    "    if promo_price_span:\n",
    "        # Use a nested find to get the sr-only span within the promo price div\n",
    "        promo_price_span_inner = promo_price_span.find('span', class_='sr-only')\n",
    "        if promo_price_span_inner:\n",
    "            # Use regular expression to extract the numeric promo price (e.g., 6.53)\n",
    "            match_promo_price = re.search(r'[\\d]+[.,][\\d]+', promo_price_span_inner.get('aria-label'))\n",
    "            promo_price = match_promo_price.group() if match_promo_price else 'Promo price not found'\n",
    "        else:\n",
    "            promo_price = 'Promo price not found'\n",
    "    else:\n",
    "        promo_price = 'Promo price not found'\n",
    "\n",
    "    # Extract the product title from the title attribute of the anchor tag\n",
    "    title_tag = article.find('a', class_='link_root__EqRHd')\n",
    "    title = title_tag.get('title') if title_tag else 'Title not found'\n",
    "    \n",
    "    # Extract the weight from the product-unit-size span\n",
    "    weight_span = article.find('span', class_='price_unitSize__Hk6E4')\n",
    "    weight = weight_span.get_text(strip=True) if weight_span else 'Weight not found'\n",
    "\n",
    "    # Store the extracted information as a tuple, including promo price\n",
    "    products.append((title, price, promo_price, weight, \"Non_Branded\", \"AH\"))\n",
    "    \n",
    "# Get current timestamp for the data\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD HH:MM:SS\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open('AH_choco.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Product Title', 'Price', 'promo_price', 'weight', 'Branded', 'Retailer', 'Timestamp'])  # Write header row\n",
    "    for product in products:\n",
    "        writer.writerow((*product, timestamp))  # Write product data with timestamp\n",
    "\n",
    "print(f\"Data has been successfully saved to products.csv\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db635d48",
   "metadata": {},
   "source": [
    "# Jumbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1879290c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 24 products.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import re\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=Options())\n",
    "\n",
    "\n",
    "# Navigate to the Jumbo products page\n",
    "url = \"https://www.jumbo.com/producten/?searchType=keyword&searchTerms=jumbo+chocolade+snoep\"\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load and accept cookies\n",
    "try:\n",
    "    accept_button = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.ID, \"onetrust-reject-all-handler\"))\n",
    "    )\n",
    "    accept_button.click()\n",
    "except:\n",
    "    print(\"No accept cookies button found.\")\n",
    "\n",
    "# Wait for products to load\n",
    "WebDriverWait(driver, 20).until(\n",
    "    EC.presence_of_all_elements_located((By.CLASS_NAME, \"jum-card\"))\n",
    ")\n",
    "\n",
    "# Load page source into BeautifulSoup\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Extract product data\n",
    "products = []\n",
    "\n",
    "for product_card in soup.find_all(\"div\", class_=\"jum-card\"):\n",
    "    # Extract product title\n",
    "    title_tag = product_card.find(\"a\", class_=\"title-link\")\n",
    "    title = title_tag.text.strip() if title_tag else \"Title not found\"\n",
    "    \n",
    "    # Extract promo price\n",
    "    promo_price_div = product_card.find(\"div\", class_=\"promo-price\")\n",
    "    promo_price = (\n",
    "        re.search(r\"[\\d]+[.,][\\d]+\", promo_price_div.text.strip()).group()\n",
    "        if promo_price_div and promo_price_div.text\n",
    "        else \"Promo price not found\"\n",
    "    )\n",
    "\n",
    "    # Extract price\n",
    "    price_whole = product_card.find(\"span\", class_=\"whole\")\n",
    "    price_fractional = product_card.find(\"span\", class_=\"fractional\")\n",
    "    price = (\n",
    "        f\"{price_whole.text.strip()},{price_fractional.text.strip()}\"\n",
    "        if price_whole and price_fractional\n",
    "        else \"Price not found\"\n",
    "    )\n",
    "\n",
    "    # Extract weight\n",
    "    subtitle_div = product_card.find(\"div\", class_=\"subtitle\")\n",
    "    weight_span = subtitle_div.find(\"span\", class_=\"text\") if subtitle_div else None\n",
    "    weight = weight_span.text.strip() if weight_span else \"Weight not found\"\n",
    "\n",
    "    # Append to products list\n",
    "    products.append((title, promo_price, price, weight, \"Non_Branded\", \"Jumbo\"))\n",
    "\n",
    "# Write to CSV\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "with open(\"choco.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Product Title\", \"Promo Price\", \"Price\", \"Weight\", 'Branded', 'Retailer', 'Timestamp'])\n",
    "    for product in products:\n",
    "        writer.writerow((*product, timestamp))\n",
    "\n",
    "print(f\"Extracted {len(products)} products.\")\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c329bc",
   "metadata": {},
   "source": [
    "# Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "346a7b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved to Plus_choco.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import csv\n",
    "import re  # Importing the regular expression module\n",
    "from datetime import datetime  # Importing datetime for timestamp\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=Options())\n",
    "\n",
    "\n",
    "url = \"https://www.plus.nl/producten/snoep-koek-chocolade-chips-noten/chocolade/chocoladesnoepjes?merk=PLUS\"\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "\n",
    "# Click the \"Weigeren\" button to reject cookies on the Plus site\n",
    "\n",
    "# Wait for the \"Weigeren\" button to be clickable\n",
    "accept_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'btn-cookies-refuse')]\")))\n",
    "accept_button.click()\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# List to store the extracted product information\n",
    "products = []\n",
    "\n",
    "# Loop through all product articles\n",
    "for article in soup.find_all('a', id=re.compile(\".*-produt_item_link\")):\n",
    "    # Extract the product title from the title attribute of the anchor tag\n",
    "    title = article.get('title', 'Title not found')\n",
    "\n",
    "    # Extract the price from the price integers and decimals\n",
    "    price_integer = article.find('div', class_='font-bold product-header-price-integer')\n",
    "    price_decimals = article.find('div', class_='font-black product-header-price-decimals')\n",
    "\n",
    "    if price_integer and price_decimals:\n",
    "        price = f\"{price_integer.get_text(strip=True)}{price_decimals.get_text(strip=True)}\"\n",
    "    else:\n",
    "        price = 'Price not found'\n",
    "\n",
    "    # Extract the previous (old) price from the price-previous div\n",
    "    previous_price_span = article.find('div', class_='product-header-price-previous')\n",
    "    if previous_price_span:\n",
    "        # Extract the old price as text\n",
    "        promo_price = previous_price_span.get_text(strip=True)\n",
    "    else:\n",
    "        promo_price = 'Promo price not found'\n",
    "\n",
    "    # Extract the weight from the 'Per 250 g' span\n",
    "    weight_span = article.find('span', class_='OSFillParent')\n",
    "    weight = weight_span.get_text(strip=True) if weight_span else 'Weight not found'\n",
    "\n",
    "    # Store the extracted information as a tuple, including promo price\n",
    "    products.append((title, price, promo_price, weight, \"Non_Branded\", \"Plus\"))\n",
    "\n",
    "# Get current timestamp for the data\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD HH:MM:SS\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open('choco.csv', mode='a', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for product in products:\n",
    "        writer.writerow((*product, timestamp))  # Write product data with timestamp\n",
    "\n",
    "print(f\"Data has been successfully saved to Plus_choco.csv\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d732faa7",
   "metadata": {},
   "source": [
    "#### Plus Rotsjes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9a8a46cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved to Plus_choco.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import csv\n",
    "import re  # Importing the regular expression module\n",
    "from datetime import datetime  # Importing datetime for timestamp\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=Options())\n",
    "\n",
    "\n",
    "url = \"https://www.plus.nl/zoekresultaten?SearchTerm=rotsjes&merk=PLUS\"\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "\n",
    "# Click the \"Weigeren\" button to reject cookies on the Plus site\n",
    "\n",
    "# Wait for the \"Weigeren\" button to be clickable\n",
    "accept_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'btn-cookies-refuse')]\")))\n",
    "accept_button.click()\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# List to store the extracted product information\n",
    "products = []\n",
    "\n",
    "# Loop through all product articles\n",
    "for article in soup.find_all('a', id=re.compile(\".*-produt_item_link\")):\n",
    "    # Extract the product title from the title attribute of the anchor tag\n",
    "    title = article.get('title', 'Title not found')\n",
    "\n",
    "    # Extract the price from the price integers and decimals\n",
    "    price_integer = article.find('div', class_='font-bold product-header-price-integer')\n",
    "    price_decimals = article.find('div', class_='font-black product-header-price-decimals')\n",
    "\n",
    "    if price_integer and price_decimals:\n",
    "        price = f\"{price_integer.get_text(strip=True)}{price_decimals.get_text(strip=True)}\"\n",
    "    else:\n",
    "        price = 'Price not found'\n",
    "\n",
    "    # Extract the previous (old) price from the price-previous div\n",
    "    previous_price_span = article.find('div', class_='product-header-price-previous')\n",
    "    if previous_price_span:\n",
    "        # Extract the old price as text\n",
    "        promo_price = previous_price_span.get_text(strip=True)\n",
    "    else:\n",
    "        promo_price = 'Promo price not found'\n",
    "\n",
    "    # Extract the weight from the 'Per 250 g' span\n",
    "    weight_span = article.find('span', class_='OSFillParent')\n",
    "    weight = weight_span.get_text(strip=True) if weight_span else 'Weight not found'\n",
    "\n",
    "    # Store the extracted information as a tuple, including promo price\n",
    "    products.append((title, price, promo_price, weight, \"Non_Branded\", \"Plus\"))\n",
    "\n",
    "# Get current timestamp for the data\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD HH:MM:SS\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open('choco.csv', mode='a', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for product in products:\n",
    "        writer.writerow((*product, timestamp))  # Write product data with timestamp\n",
    "\n",
    "print(f\"Data has been successfully saved to Plus_choco.csv\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d8b30c",
   "metadata": {},
   "source": [
    "# Dirk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2b8be82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved to choco.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import csv\n",
    "import re  # Importing the regular expression module\n",
    "from datetime import datetime  # Importing datetime for timestamp\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=Options())\n",
    "\n",
    "\n",
    "url = \"https://www.dirk.nl/boodschappen/snacks-snoep/chocolade\"\n",
    "driver.get(url)\n",
    "time.sleep(10)\n",
    "\n",
    "# Function to automatically scroll down the page\n",
    "def scroll_to_load_more(driver, wait_time=2, scroll_increment=700, scroll_limit=1):\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    for _ in range(scroll_limit):\n",
    "        # Scroll down by the specified increment\n",
    "        driver.execute_script(f\"window.scrollBy(0, {scroll_increment});\")\n",
    "        \n",
    "        # Wait for new products to load\n",
    "        time.sleep(wait_time)\n",
    "        \n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break  # Exit the loop if no new content is loaded\n",
    "        last_height = new_height\n",
    "        \n",
    "scroll_to_load_more(driver, wait_time=2, scroll_increment=700, scroll_limit=1)\n",
    "\n",
    "# Wait for the label for \"Overige chocolade & bonbons\" to be clickable and click it\n",
    "filter_label = WebDriverWait(driver, 10).until(\n",
    "EC.element_to_be_clickable((By.XPATH, \"//label[contains(text(), 'Overige chocolade & bonbons')]\")))\n",
    "filter_label.click()  # Click the label to apply the filter\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Wait for the label for \"1 de beste\" to be clickable and click it\n",
    "filter_label = WebDriverWait(driver, 10).until(\n",
    "EC.element_to_be_clickable((By.XPATH, \"//label[contains(text(), '1 de Beste')]\")))\n",
    "filter_label.click()  # Click the label to apply the filter\n",
    "\n",
    "# Wait for the products to load after filtering\n",
    "time.sleep(5)\n",
    "\n",
    "# Wait for a specific element that indicates products are loaded\n",
    "WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.XPATH, \"//article[@data-product-id]\"))\n",
    ")\n",
    "\n",
    "# Now scrape the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# List to store the extracted product information\n",
    "products = []\n",
    "\n",
    "# Loop through all product articles\n",
    "for article in soup.find_all('article', attrs={'data-product-id': True}):\n",
    "    # Extract the product title from the 'title' class\n",
    "    title = article.find('p', class_='title').get_text(strip=True) if article.find('p', class_='title') else 'Title not found'\n",
    "\n",
    "    # Extract the price from the 'price' class\n",
    "    price_integer = article.find('span', class_='price-large')\n",
    "    price_decimals = article.find('span', class_='price-small')\n",
    "\n",
    "    if price_integer and price_decimals:\n",
    "        price = f\"{price_integer.get_text(strip=True)},{price_decimals.get_text(strip=True)}\"\n",
    "    else:\n",
    "        price = 'Price not found'\n",
    "\n",
    "    # Extract the promotional price\n",
    "    promo_price_span = article.find('div', class_='label price-label')\n",
    "    if promo_price_span:\n",
    "        # Get the actual promotional price which is nested inside\n",
    "        promo_price = promo_price_span.find('span', class_='regular-price').find('span').get_text(strip=True)\n",
    "    else:\n",
    "        promo_price = 'Promo price not found'\n",
    "\n",
    "    # Extract the weight from the subtitle span\n",
    "    weight_span = article.find('span', class_='subtitle')\n",
    "    weight = weight_span.get_text(strip=True) if weight_span else 'Weight not found'\n",
    "\n",
    "    # Store the extracted information as a tuple, including promo price\n",
    "    products.append((title, price, promo_price, weight, \"Non_Branded\", \"Dirk\"))\n",
    "\n",
    "# Get current timestamp for the data\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD HH:MM:SS\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open('choco.csv', mode='a', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for product in products:\n",
    "        writer.writerow((*product, timestamp))  # Write product data with timestamp\n",
    "\n",
    "print(\"Data has been successfully saved to choco.csv\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6584ea",
   "metadata": {},
   "source": [
    "#### Dirk Rotsjes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cf30d880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved to choco.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import csv\n",
    "import re  # Importing the regular expression module\n",
    "from datetime import datetime  # Importing datetime for timestamp\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=Options())\n",
    "\n",
    "\n",
    "url = \"https://www.dirk.nl/zoeken/producten/chocolade%20rotsjes\"\n",
    "driver.get(url)\n",
    "time.sleep(10)\n",
    "\n",
    "\n",
    "# Now scrape the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# List to store the extracted product information\n",
    "products = []\n",
    "\n",
    "# Loop through all product articles\n",
    "for article in soup.find_all('article', attrs={'data-product-id': True}):\n",
    "    # Extract the product title from the 'title' class\n",
    "    title = article.find('p', class_='title').get_text(strip=True) if article.find('p', class_='title') else 'Title not found'\n",
    "\n",
    "    # Extract the price from the 'price' class\n",
    "    price_integer = article.find('span', class_='price-large')\n",
    "    price_decimals = article.find('span', class_='price-small')\n",
    "\n",
    "    if price_integer and price_decimals:\n",
    "        price = f\"{price_integer.get_text(strip=True)},{price_decimals.get_text(strip=True)}\"\n",
    "    else:\n",
    "        price = 'Price not found'\n",
    "\n",
    "    # Extract the promotional price\n",
    "    promo_price_span = article.find('div', class_='label price-label')\n",
    "    if promo_price_span:\n",
    "        # Get the actual promotional price which is nested inside\n",
    "        promo_price = promo_price_span.find('span', class_='regular-price').find('span').get_text(strip=True)\n",
    "    else:\n",
    "        promo_price = 'Promo price not found'\n",
    "\n",
    "    # Extract the weight from the subtitle span\n",
    "    weight_span = article.find('span', class_='subtitle')\n",
    "    weight = weight_span.get_text(strip=True) if weight_span else 'Weight not found'\n",
    "\n",
    "    # Store the extracted information as a tuple, including promo price\n",
    "    products.append((title, price, promo_price, weight, \"Non_Branded\", \"Dirk\"))\n",
    "\n",
    "# Get current timestamp for the data\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD HH:MM:SS\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open('choco.csv', mode='a', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for product in products:\n",
    "        writer.writerow((*product, timestamp))  # Write product data with timestamp\n",
    "\n",
    "print(\"Data has been successfully saved to choco.csv\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dc77d3",
   "metadata": {},
   "source": [
    "# Vomar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "15f4f30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No accept cookies button found.\n",
      "Data has been successfully saved to choco.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import csv\n",
    "import re  # Importing the regular expression module\n",
    "from datetime import datetime  # Importing datetime for timestamp\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=Options())\n",
    "\n",
    "\n",
    "url = \"https://www.vomar.nl/zoeken?search=g%27woon%20choco\"\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "\n",
    "# Click the \"Weigeren\" button to reject cookies on the Plus site\n",
    "\n",
    "# Wait for the \"Deny\" button to be clickable\n",
    "\n",
    "\n",
    "try:\n",
    "    deny_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, \"CybotCookiebotDialogBodyButtonDecline\")))\n",
    "    deny_button.click()\n",
    "except:\n",
    "    print(\"No accept cookies button found.\")\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "products = []\n",
    "\n",
    "# Loop through all product articles\n",
    "for article in soup.find_all('div', class_='col-xs-12 col-md-3 product'):\n",
    "    # Extract the product title from the 'description' class\n",
    "    title = article.find('p', class_='description').get_text(strip=True) if article.find('p', class_='description') else 'Title not found'\n",
    "\n",
    "    # Extract the price from the 'price right' class\n",
    "    price_integer = article.find('span', class_='large')\n",
    "    price_decimals = article.find('span', class_='small')\n",
    "\n",
    "    if price_integer and price_decimals:\n",
    "        price = f\"{price_integer.get_text(strip=True)}{price_decimals.get_text(strip=True)}\"\n",
    "    else:\n",
    "        price = 'Price not found'\n",
    "\n",
    "    # Extract the promotional price (if applicable, based on previous logic)\n",
    "    # Adjust according to actual structure; this section may be removed if not needed\n",
    "    promo_price = 'Promo price not found'  # Placeholder since no promo price was in the provided HTML\n",
    "\n",
    "    # Weight extraction can be omitted as there is no weight data in the provided HTML\n",
    "    weight = 'Weight not found'  # Placeholder since no weight was provided\n",
    "\n",
    "    # Store the extracted information as a tuple\n",
    "    products.append((title, price, promo_price, weight, \"Non_Branded\", \"Vomar\"))\n",
    "\n",
    "# Get current timestamp for the data\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD HH:MM:SS\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open('choco.csv', mode='a', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for product in products:\n",
    "        writer.writerow((*product, timestamp))  # Write product data with timestamp\n",
    "\n",
    "print(\"Data has been successfully saved to choco.csv\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560f9f42",
   "metadata": {},
   "source": [
    "# Aldi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8b4ea634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved to choco.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=Options())\n",
    "\n",
    "# Setup Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "\n",
    "# URL of the Aldi search page\n",
    "url = \"https://www.aldi.nl/zoeken.html?query=time4choco&searchCategory=Submitted%20Search&indices%5Bprod_nl_nl_assortment%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_assortment%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_offers%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_offers%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_recipes%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_recipes%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&indices%5Bprod_nl_nl_content%5D%5Bconfigure%5D%5BclickAnalytics%5D=true&indices%5Bprod_nl_nl_content%5D%5Bconfigure%5D%5BhitsPerPage%5D=12&configure%5BclickAnalytics%5D=true\"\n",
    "driver.get(url)\n",
    "\n",
    "# Use WebDriverWait to wait for the articles to load\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"mod-article-tile--default\")))\n",
    "\n",
    "# Get the page source after the JavaScript has rendered the HTML\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# List to hold product data\n",
    "products = []\n",
    "\n",
    "# Loop through all product articles\n",
    "for article in soup.find_all('div', class_='mod-article-tile mod-article-tile--default'):\n",
    "    # Extract the product title\n",
    "    title = article.find('span', class_='mod-article-tile__title').get_text(strip=True) if article.find('span', class_='mod-article-tile__title') else 'Title not found'\n",
    "\n",
    "    # Extract the promotional price if it exists\n",
    "    promo_price_element = article.find('s', class_='price__previous')\n",
    "    promo_price = promo_price_element.get_text(strip=True) if promo_price_element else 'Promo price not found'\n",
    "\n",
    "    # Extract the current price\n",
    "    current_price_element = article.find('span', class_='price__wrapper')\n",
    "    current_price = current_price_element.get_text(strip=True) if current_price_element else 'Price not found'\n",
    "\n",
    "    # Extract the weight\n",
    "    weight = article.find('span', class_='price__unit').get_text(strip=True) if article.find('span', class_='price__unit') else 'Weight not found'\n",
    "\n",
    "    # Store the extracted information as a tuple\n",
    "    products.append((title, current_price, promo_price, weight, \"Non_Branded\", \"Aldi\"))\n",
    "\n",
    "# Get current timestamp for the data\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD HH:MM:SS\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open('choco.csv', mode='a', newline='', encoding='utf-8') as file:  # Use 'a' for appending\n",
    "    writer = csv.writer(file)\n",
    "    # Write header only if the file is empty\n",
    "    if file.tell() == 0:\n",
    "        writer.writerow(['Title', 'Price', 'Promo Price', 'Weight', 'Timestamp', \"Non_Branded\", \"Aldi\"])  # CSV header\n",
    "    for product in products:\n",
    "        writer.writerow((*product, timestamp))  # Write product data with timestamp\n",
    "\n",
    "print(\"Data has been successfully saved to choco.csv\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ad8ee0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved to choco.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=Options())\n",
    "\n",
    "# Setup Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "\n",
    "# URL of the Aldi search page\n",
    "url = \"https://www.aldi.nl/zoeken.html?query=rotsjes&searchCategory=Submitted%20Search\"\n",
    "driver.get(url)\n",
    "\n",
    "# Use WebDriverWait to wait for the articles to load\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"mod-article-tile--default\")))\n",
    "\n",
    "# Get the page source after the JavaScript has rendered the HTML\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# List to hold product data\n",
    "products = []\n",
    "\n",
    "# Loop through all product articles\n",
    "for article in soup.find_all('div', class_='mod-article-tile mod-article-tile--default'):\n",
    "    # Extract the product title\n",
    "    title = article.find('span', class_='mod-article-tile__title').get_text(strip=True) if article.find('span', class_='mod-article-tile__title') else 'Title not found'\n",
    "\n",
    "    # Extract the promotional price if it exists\n",
    "    promo_price_element = article.find('s', class_='price__previous')\n",
    "    promo_price = promo_price_element.get_text(strip=True) if promo_price_element else 'Promo price not found'\n",
    "\n",
    "    # Extract the current price\n",
    "    current_price_element = article.find('span', class_='price__wrapper')\n",
    "    current_price = current_price_element.get_text(strip=True) if current_price_element else 'Price not found'\n",
    "\n",
    "    # Extract the weight\n",
    "    weight = article.find('span', class_='price__unit').get_text(strip=True) if article.find('span', class_='price__unit') else 'Weight not found'\n",
    "\n",
    "    # Store the extracted information as a tuple\n",
    "    products.append((title, current_price, promo_price, weight, \"Non_Branded\", \"Aldi\"))\n",
    "\n",
    "# Get current timestamp for the data\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD HH:MM:SS\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open('choco.csv', mode='a', newline='', encoding='utf-8') as file:  # Use 'a' for appending\n",
    "    writer = csv.writer(file)\n",
    "    # Write header only if the file is empty\n",
    "    if file.tell() == 0:\n",
    "        writer.writerow(['Title', 'Price', 'Promo Price', 'Weight', 'Timestamp', \"Non_Branded\", \"Aldi\"])  # CSV header\n",
    "    for product in products:\n",
    "        writer.writerow((*product, timestamp))  # Write product data with timestamp\n",
    "\n",
    "print(\"Data has been successfully saved to choco.csv\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b165f71",
   "metadata": {},
   "source": [
    "## M&M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03f4288",
   "metadata": {},
   "source": [
    "### AH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd810bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved to products.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import csv\n",
    "import re  # Importing the regular expression module\n",
    "from datetime import datetime  # Importing datetime for timestamp\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=Options())\n",
    "\n",
    "\n",
    "url = \"https://www.ah.nl/producten/snoep-chocolade-koek/chocolade/chocoladesnoepjes?merk=M%26M%27S\"\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "\n",
    "# Borrowed the 'very cool :)' accepteer cookies button from TOTO scraper(group assignment)\n",
    "accept_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, \"decline-cookies\")))\n",
    "accept_button.click()\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# List to store the extracted product information\n",
    "products = []\n",
    "\n",
    "# Loop through all product articles\n",
    "for article in soup.find_all('article', class_='product-card-portrait_root__ZiRpZ'):\n",
    "    # Extract the price from the aria-label of the sr-only span\n",
    "    price_span = article.find('span', class_='sr-only')\n",
    "    if price_span:\n",
    "        # Use regular expression to extract the numeric price (e.g., 1.99)\n",
    "        match = re.search(r'[\\d]+[.,][\\d]+', price_span.get('aria-label'))\n",
    "        price = match.group() if match else 'Price not found'\n",
    "    else:\n",
    "        price = 'Price not found'\n",
    "        \n",
    "    # Extract the promo price (if available) from the correct div\n",
    "    promo_price_span = article.find('div', class_='price-amount_highlight__ekL92')\n",
    "    if promo_price_span:\n",
    "        # Use a nested find to get the sr-only span within the promo price div\n",
    "        promo_price_span_inner = promo_price_span.find('span', class_='sr-only')\n",
    "        if promo_price_span_inner:\n",
    "            # Use regular expression to extract the numeric promo price (e.g., 6.53)\n",
    "            match_promo_price = re.search(r'[\\d]+[.,][\\d]+', promo_price_span_inner.get('aria-label'))\n",
    "            promo_price = match_promo_price.group() if match_promo_price else 'Promo price not found'\n",
    "        else:\n",
    "            promo_price = 'Promo price not found'\n",
    "    else:\n",
    "        promo_price = 'Promo price not found'\n",
    "\n",
    "    # Extract the product title from the title attribute of the anchor tag\n",
    "    title_tag = article.find('a', class_='link_root__EqRHd')\n",
    "    title = title_tag.get('title') if title_tag else 'Title not found'\n",
    "    \n",
    "    # Extract the weight from the product-unit-size span\n",
    "    weight_span = article.find('span', class_='price_unitSize__Hk6E4')\n",
    "    weight = weight_span.get_text(strip=True) if weight_span else 'Weight not found'\n",
    "\n",
    "    # Store the extracted information as a tuple, including promo price\n",
    "    products.append((title, price, promo_price, weight,\"Branded\", \"AH\"))\n",
    "    \n",
    "# Get current timestamp for the data\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD HH:MM:SS\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open('AH_choco.csv', mode='a', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for product in products:\n",
    "        writer.writerow((*product, timestamp))  # Write product data with timestamp\n",
    "\n",
    "print(f\"Data has been successfully saved to products.csv\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf49dbe9",
   "metadata": {},
   "source": [
    "### Jumbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "455753b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 17 products.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import re\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=Options())\n",
    "\n",
    "\n",
    "# Navigate to the Jumbo products page\n",
    "url = \"https://www.jumbo.com/producten/menms/?searchType=keyword&searchTerms=m%26m\"\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load and accept cookies\n",
    "try:\n",
    "    accept_button = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.ID, \"onetrust-reject-all-handler\"))\n",
    "    )\n",
    "    accept_button.click()\n",
    "except:\n",
    "    print(\"No accept cookies button found.\")\n",
    "\n",
    "# Wait for products to load\n",
    "WebDriverWait(driver, 20).until(\n",
    "    EC.presence_of_all_elements_located((By.CLASS_NAME, \"jum-card\"))\n",
    ")\n",
    "\n",
    "# Load page source into BeautifulSoup\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Extract product data\n",
    "products = []\n",
    "\n",
    "for product_card in soup.find_all(\"div\", class_=\"jum-card\"):\n",
    "    # Extract product title\n",
    "    title_tag = product_card.find(\"a\", class_=\"title-link\")\n",
    "    title = title_tag.text.strip() if title_tag else \"Title not found\"\n",
    "    \n",
    "    # Extract promo price\n",
    "    promo_price_div = product_card.find(\"div\", class_=\"promo-price\")\n",
    "    promo_price = (\n",
    "        re.search(r\"[\\d]+[.,][\\d]+\", promo_price_div.text.strip()).group()\n",
    "        if promo_price_div and promo_price_div.text\n",
    "        else \"Promo price not found\"\n",
    "    )\n",
    "\n",
    "    # Extract price\n",
    "    price_whole = product_card.find(\"span\", class_=\"whole\")\n",
    "    price_fractional = product_card.find(\"span\", class_=\"fractional\")\n",
    "    price = (\n",
    "        f\"{price_whole.text.strip()},{price_fractional.text.strip()}\"\n",
    "        if price_whole and price_fractional\n",
    "        else \"Price not found\"\n",
    "    )\n",
    "\n",
    "    # Extract weight\n",
    "    subtitle_div = product_card.find(\"div\", class_=\"subtitle\")\n",
    "    weight_span = subtitle_div.find(\"span\", class_=\"text\") if subtitle_div else None\n",
    "    weight = weight_span.text.strip() if weight_span else \"Weight not found\"\n",
    "\n",
    "    # Append to products list\n",
    "    products.append((title, promo_price, price, weight,\"branded\", \"Jumbo\"))\n",
    "\n",
    "# Write to CSV\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "with open(\"choco.csv\", mode=\"a\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    for product in products:\n",
    "        writer.writerow((*product, timestamp))\n",
    "\n",
    "print(f\"Extracted {len(products)} products.\")\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73a2a49",
   "metadata": {},
   "source": [
    "### Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "11a4dbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved to Plus_choco.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import csv\n",
    "import re  # Importing the regular expression module\n",
    "from datetime import datetime  # Importing datetime for timestamp\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Initialize Chrome driver with Service\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=Options())\n",
    "\n",
    "\n",
    "url = \"https://www.plus.nl/producten/snoep-koek-chocolade-chips-noten/chocolade/chocoladesnoepjes?merk=M%26M%27S\"\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "\n",
    "# Click the \"Weigeren\" button to reject cookies on the Plus site\n",
    "\n",
    "# Wait for the \"Weigeren\" button to be clickable\n",
    "accept_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'btn-cookies-refuse')]\")))\n",
    "accept_button.click()\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# List to store the extracted product information\n",
    "products = []\n",
    "\n",
    "# Loop through all product articles\n",
    "for article in soup.find_all('a', id=re.compile(\".*-produt_item_link\")):\n",
    "    # Extract the product title from the title attribute of the anchor tag\n",
    "    title = article.get('title', 'Title not found')\n",
    "\n",
    "    # Extract the price from the price integers and decimals\n",
    "    price_integer = article.find('div', class_='font-bold product-header-price-integer')\n",
    "    price_decimals = article.find('div', class_='font-black product-header-price-decimals')\n",
    "\n",
    "    if price_integer and price_decimals:\n",
    "        price = f\"{price_integer.get_text(strip=True)}{price_decimals.get_text(strip=True)}\"\n",
    "    else:\n",
    "        price = 'Price not found'\n",
    "\n",
    "    # Extract the previous (old) price from the price-previous div\n",
    "    previous_price_span = article.find('div', class_='product-header-price-previous')\n",
    "    if previous_price_span:\n",
    "        # Extract the old price as text\n",
    "        promo_price = previous_price_span.get_text(strip=True)\n",
    "    else:\n",
    "        promo_price = 'Promo price not found'\n",
    "\n",
    "    # Extract the weight from the 'Per 250 g' span\n",
    "    weight_span = article.find('span', class_='OSFillParent')\n",
    "    weight = weight_span.get_text(strip=True) if weight_span else 'Weight not found'\n",
    "\n",
    "    # Store the extracted information as a tuple, including promo price\n",
    "    products.append((title, price, promo_price, weight, \"branded\", \"Plus\"))\n",
    "\n",
    "# Get current timestamp for the data\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d')  # Format: YYYY-MM-DD HH:MM:SS\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open('choco.csv', mode='a', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for product in products:\n",
    "        writer.writerow((*product, timestamp))  # Write product data with timestamp\n",
    "\n",
    "print(f\"Data has been successfully saved to Plus_choco.csv\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
